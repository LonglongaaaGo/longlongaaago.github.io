---

title: "Diverse facial inpainting guided by exemplars"
collection: publications
permalink: /publication/exe-gan
excerpt: ' **Wanglong Lu**, [Hanli Zhao](http://i3s.wzu.edu.cn/info/1104/1183.htm), [Xianta Jiang](http://www.cs.mun.ca/~xiantaj/), [Xiaogang Jin](http://www.cad.zju.edu.cn/home/jin/), Min Wang, Jiankai Lyu, and Kaijie Shi'
date: 2022-02-13
venue: 'Coming soon'
paperurl: 'https://arxiv.org/abs/2202.06358'

---

![results](https://longlongaaago.github.io/images/publications/exe_celeba_diverse.png)
<b> Brief description:</b>
<div style="text-align: justify"> Facial image inpainting is a task of filling visually realistic and semantically meaningful contents for missing or masked pixels in a face image. Although existing methods have made significant progress in achieving high visual quality, the controllable diversity of facial image inpainting remains an open problem in this field. This paper introduces EXE-GAN, a novel diverse and interactive facial inpainting framework, which can not only preserve the high-quality visual effect of the whole image but also complete the face image with exemplar-like facial attributes. The proposed facial inpainting is achieved based on generative adversarial networks by leveraging the global style of input image, the stochastic style, and the exemplar style of exemplar image. A novel attribute similarity metric is introduced to encourage networks to learn the style of facial attributes from the exemplar in a self-supervised way. To guarantee the natural transition across the boundary of inpainted regions, a novel spatial variant gradient backpropagation technique is designed to adjust the loss gradients based on the spatial location. A variety of experimental results and comparisons on public CelebA-HQ and FFHQ datasets are presented to demonstrate the superiority of the proposed method in terms of both the quality and diversity in facial inpainting. </div>


[[Homepage]](https://longlongaaago.github.io/EXE-GAN/)


Recommended citation: 

```
coming soon
```
